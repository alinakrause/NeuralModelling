{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part of the assignment is to introduce you to the concept of Dyna (Sutton, 1990). By now you will have heard about model-free (MF) and model-based (MB) approaches to control. MF algorithms learn and store estimates of the state-action value function. This means that control simply involves retrieving those cached value estimates and choosing among the available actions by comparing their worth. MB control, on the other hand, involves the use of a model to calculate those values. In the RL literature this is typically known as planning. Planning has the advantage of affording behavioural flexibility, since once a change in the world is discovered the model can be updated and the values re-calculated in a way that reflects the global knowledge of the environment. By contrast, MF algorithms need many experiences to propagate the information about the change to other states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyna (Sutton, 1990) is an integrated architecture which combines the merits of MF and MB approaches. Whilst interacting with the environment (i.e., being online), Dyna learns MF state-action values as well as a model of its environment (in the most general sense this involves both the transition and reward models). Whilst not interacting with the environment (i.e., being offline -- this can be in between consecutive moves or episodes, or during the equivalent of sleep in animals), Dyna uses its learnt model to additinally train the MF values. That is, the model now acts as a simulator of the environment and provides additional experiences for learning. This means that at decision time Dyna is fast to react, since it acts according to an MF policy by simply retrieving the relevant values; however, those values have been trained by a model and therefore contain some portion of the global knowledge of the environment collected so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly critical aspect of learning is exploration, and in fact Dyna's original motivation was to improve the efficiency of exploration. You are invited to read the original paper by Sutton (1990) to familiarise yourself with the idea and the sort of problems it attempts to solve. The pdf of the paper can be found in the `papers` folder of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the assignment is to reproduce some of the results from the original Dyna paper (Sutton, 1990). In particular, **your task is to generate and visualise data plotted in figure 6** in that paper. You can neglect Dyna-PI and only implement Dyna-$Q$- and Dyna-$Q$+.\n",
    "\n",
    "To make your life a little easier, and to let you jump right into the more interesting stuff, you are provided with the environment simulator located in `environment.py`, as well as a blueprint of the main code for the agent. That is, you have access to the file `agent.py` where you will find the `DynaAgent` class. This class has a method called `simulate` with the main simulation loop already implemented.\n",
    "\n",
    "**Your task is to fill in the missing implementation** in the `agent.py` file. Thus, you are tasked to complete the following functions:\n",
    "- `_policy`. This is the typical $\\pi(a\\mid s)$ which specifies how the agent chooses actions in any given state\n",
    "- `_update_qvals`. This is the $Q$-value update rule\n",
    "- `_update_experience_buffer`. This updates the agent's experience buffer from which it then samples planning updates\n",
    "- `_update_action_count`. This counts the number of moves elapsed since each action has last been attempted\n",
    "- `_plan`. This is the function which lets the agent plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done you can run the code below which will hopefully reproduce the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from write_load import load_env\n",
    "from agent import DynaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environments\n",
    "maze_conf_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'envs'))\n",
    "maze1_conf = load_env(os.path.join(maze_conf_path, 'dyna1.txt')) # maze with only the right path open \n",
    "maze2_conf = load_env(os.path.join(maze_conf_path, 'dyna2.txt')) # maze with both paths open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the agents\n",
    "# note that alpha is the learning rate (instead of beta as in the paper)\n",
    "dyna_qplus  = DynaAgent(alpha=0.5, gamma=0.9, epsilon=0.001)\n",
    "dyna_qminus = DynaAgent(alpha=0.5, gamma=0.9, epsilon=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "# run simulations\n",
    "num_trials = 3000\n",
    "num_runs   = 50\n",
    "perf       = [np.zeros((num_runs, num_trials*2)), np.zeros((num_runs, num_trials*2))]\n",
    "agents     = [dyna_qplus, dyna_qminus]\n",
    "\n",
    "for idx_agent, agent in enumerate(agents):\n",
    "    for idx_run in range(num_runs):\n",
    "        agent.init_env(**maze1_conf)\n",
    "        agent.simulate(num_trials=num_trials, reset_agent=True, num_planning_updates=10)\n",
    "        # world change\n",
    "        agent.init_env(**maze2_conf)\n",
    "        agent.simulate(num_trials=num_trials, reset_agent=False, num_planning_updates=10)\n",
    "        # save performance\n",
    "        perf[idx_agent][idx_run, :] = agent.get_performace()\n",
    "        if (idx_run+1)%10 == 0:\n",
    "            print('done with run %u/%u'%(idx_run+1, num_runs))\n",
    "\n",
    "# average cumulative reward\n",
    "avg_perf_dyna_qplus  = np.mean(perf[0], axis=0)\n",
    "avg_perf_dyna_qminus = np.mean(perf[1], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Figure 6 from Sutton (1990) here\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(avg_perf_dyna_qplus,  label='Dyna-Q+')\n",
    "plt.plot(avg_perf_dyna_qminus, label='Dyna-Q-')\n",
    "plt.axvline(3000, linestyle='--', c='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in your own words the apparent differences you observe in the above plot. Some of those differences involve the particular choice of the exploration bonus used in this algorithm. Suggest and implement another sensible exploration bonus and compare the performance of your agent against Dyna-$Q$+ and Dyna-$Q$-. Explain your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-step task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2-step task is one of the most iconic RL tasks (Daw et al. 2011). It was designed to dissect the relative contributions of the MF and MB systems in human choices. There are multiple ways in which MB information can enter choice. For instance, as you will have seen in the case of Dyna, the MF values are additionally trained by the MB system during offline behavioural states. In fact, this process of Dyna-style planning parallels closely hippocampal replay which has been suggested to implement MB planning (Mattar \\& Daw, 2018).\n",
    "\n",
    "For the purpose of this exercise, we will assume that the choice is guided by a linear combination of the MF and MB values. Thus, by tweaking the relative contribution of each, you would expect different behaviours to emerge. Classically, the measure of this balance used in the 2-step task is stay probability. That is, the probability that the subject/agent repeats the same first-stage choice conditioned on the outcome of the second stage in the previous trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 [20 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assigniment, **your task is to reproduce and visualise data plotted in figure 2** in Daw et al. (2011). There is no pre-implemented code for the agent, except for some basics in the `agent.py` file where you can find the `TwoStepAgent` class. Therefore, you have to follow the methodology in the paper and implement it yourself. You can find the relevant paper in the `papers` folder of this git repository. The only provided code is the one below, as well as the `get_stay_probabilities` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import TwoStepAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the agent. The parameters are taken from the paper\n",
    "agent = TwoStepAgent(alpha1=0.54, alpha2=0.42, beta1=5.19, beta2=3.69, lam=0.57, w=0.39, p=0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "# run simulations\n",
    "num_trials  = 201\n",
    "num_averg   = 17\n",
    "stay_probas = np.zeros((num_averg, 4))\n",
    "for n in range(num_averg):\n",
    "    agent.simulate(num_trials)\n",
    "    stay_probas[n, :] = agent.get_stay_probabilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(1, np.mean(stay_probas[:, 0]), facecolor='blue', label='common')\n",
    "plt.bar(2, np.mean(stay_probas[:, 1]), facecolor='red', label='rare')\n",
    "plt.bar(3, np.mean(stay_probas[:, 2]), facecolor='blue')\n",
    "plt.bar(4, np.mean(stay_probas[:, 3]), facecolor='red')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xticks([1.5, 3.5], ['rewarded', 'unrewarded'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in your own words the apparent differences between the MF and MB agents. What do the data plotted with best-fitting parameters tell you about the relative contributions of MF and MB systems to subjects' choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
