{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dx/zswlyk812zq8m7h3crzgxx9r0000gn/T/ipykernel_50808/3290623192.py:105: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.exp(x).sum()\n",
      "/var/folders/dx/zswlyk812zq8m7h3crzgxx9r0000gn/T/ipykernel_50808/3290623192.py:105: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / np.exp(x).sum()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb#W5sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39marange(\u001b[39m3\u001b[39m), p\u001b[39m=\u001b[39magent_normal\u001b[39m.\u001b[39mpolicy[state])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb#W5sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb#W5sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(np\u001b[39m.\u001b[39;49marange(\u001b[39m2\u001b[39;49m), p\u001b[39m=\u001b[39;49magent_normal\u001b[39m.\u001b[39;49mpolicy[state])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb#W5sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m reward \u001b[39m=\u001b[39m env_normal\u001b[39m.\u001b[39mtake_step(action)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alinakrause/Documents/GitHub/NeuralModelling/HW2/actor_critic.ipynb#W5sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m new_state \u001b[39m=\u001b[39m env_normal\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:970\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BaseEnvironment:\n",
    "    def __init__(self) -> None:\n",
    "        self.state = 0\n",
    "        self.terminated = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        self.terminated = True\n",
    "\n",
    "    def new_ep(self):\n",
    "        self.terminated = False\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def take_step(self, action):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "class NormalEnvironment(BaseEnvironment):\n",
    "    def get_reward(self, action):\n",
    "        if self.state == 0:\n",
    "            reward = 0\n",
    "        elif self.state == 1:\n",
    "            rewards = [4, 0]\n",
    "            reward = rewards[action]\n",
    "        elif self.state == 2:\n",
    "            rewards = [2, 3]\n",
    "            reward = rewards[action]\n",
    "        return reward\n",
    "\n",
    "    def take_step(self, action):\n",
    "        self.terminated = False\n",
    "        reward = self.get_reward(action)\n",
    "        if self.state == 0:\n",
    "                if action < 2:\n",
    "                    # there is no further state\n",
    "                    self.reset()\n",
    "                else:\n",
    "                    #random transition to left or right if C is chosen\n",
    "                    self.state = np.random.randint(1,3)\n",
    "        else:\n",
    "                # there is no further state\n",
    "            self.reset()\n",
    "        return reward\n",
    "\n",
    "class MazeEnvironment(BaseEnvironment):\n",
    "    def get_reward(self, action):\n",
    "        if self.state == 0:\n",
    "            reward = 0\n",
    "        elif self.state == 1:\n",
    "            rewards = [4,0]\n",
    "            reward = rewards[action]\n",
    "        elif self.state == 2:\n",
    "            rewards = [2,3]\n",
    "            reward = rewards[action]\n",
    "        return reward\n",
    "\n",
    "    def take_step(self,  action):\n",
    "        self.terminated = False\n",
    "        reward = self.get_reward(action)\n",
    "        if self.state == 0:\n",
    "            self.state = self.state + 1 + action\n",
    "        else:\n",
    "            # there is no further state\n",
    "            self.reset()\n",
    "        return reward\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, env, epsilon_a=0.075, policy_type=\"random\", epsilon_c=0.2, decay=0, beta=1) -> None:\n",
    "        self.vs = np.zeros(3)\n",
    "        #self.policy = [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
    "        self.policy_type = policy_type\n",
    "        self.env = env\n",
    "        self.epsilon_a = epsilon_a\n",
    "        self.epsilon_c = epsilon_c\n",
    "        self.decay = decay\n",
    "        self.beta = beta\n",
    "\n",
    "    def reset(self):\n",
    "        # reset at the start of a new epoch\n",
    "        #self.vs = np.zeros(3)\n",
    "        #self.policy = [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def critic(self):\n",
    "        return self.vs\n",
    "\n",
    "    def actor(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def update_policy(self, state, reward, new_state, action):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def update_weights(self, reward, state, new_state):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
    "\n",
    "    def softmax(self, x):\n",
    "        for i in range(len(x)):\n",
    "            x[i] = self.beta * x[i]\n",
    "        return np.exp(x) / np.exp(x).sum()\n",
    "\n",
    "    def delta_func(self, a, b):\n",
    "        return a == b\n",
    "\n",
    "class ChoiceAgent(BaseAgent):\n",
    "    def __init__(self, env, epsilon_a=0.075, policy_type=\"random\", epsilon_c=0.2, decay=0, beta=1) -> None:\n",
    "        super().__init__(env, epsilon_a, policy_type, epsilon_c, decay, beta)\n",
    "        self.ms = [[0, 0, 0], [0, 0], [0, 0]]\n",
    "        self.policy = [[1/3, 1/3, 1/3], [0.5, 0.5], [0.5, 0.5]]\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset at the start of a new epoch\n",
    "        self.values = np.zeros(3)\n",
    "        self.memory = [[0, 0, 0], [0, 0], [0, 0]]\n",
    "        self.policy = [[1/3, 1/3, 1/3], [0.5, 0.5], [0.5, 0.5]]\n",
    "\n",
    "    def actor(self):\n",
    "        state = self.env.get_state()\n",
    "        if state == 0:\n",
    "            action = np.random.choice(np.arange(3), p=self.policy[state])\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(2), p=self.policy[state][0:2])  # Adjusted here\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, state, reward, new_state, action):\n",
    "        if action < 2:\n",
    "            delta = reward - self.vs[state]\n",
    "        else:\n",
    "            delta = reward + self.vs[new_state] - self.vs[state]\n",
    "        for a in range(len(self.ms[state])):\n",
    "            self.ms[state][a] = (1 - self.decay) * self.ms[state][a] + self.epsilon_a * self.delta_func(action, a) * delta\n",
    "            self.policy[state] = self.softmax(self.ms[state])\n",
    "        return self.policy\n",
    "\n",
    "    def update_weights(self, reward, state, new_state):\n",
    "        if new_state != 0:\n",
    "            delta = reward + self.vs[new_state] - self.vs[state]\n",
    "        else:\n",
    "            delta = reward - self.vs[state]\n",
    "\n",
    "        self.vs[state] += self.epsilon_c * delta\n",
    "\n",
    "        return self.vs\n",
    "\n",
    "class MazeAgent(BaseAgent):\n",
    "    def __init__(self, env, epsilon_a=0.075, policy_type=\"random\", epsilon_c=0.2, decay=0, beta=1) -> None:\n",
    "        super().__init__(env, epsilon_a, policy_type, epsilon_c, decay, beta)\n",
    "        self.ms = [[0, 0], [0, 0], [0, 0]]\n",
    "        self.policy = [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]\n",
    "\n",
    "    def actor(self):\n",
    "        state = self.env.get_state()\n",
    "        action = np.random.choice(np.arange(2), p=self.policy[state])\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, state, reward, new_state, action):\n",
    "        if state == 0:\n",
    "            delta = reward + self.vs[new_state] - self.vs[state]\n",
    "        else:\n",
    "            delta = reward - self.vs[state]\n",
    "        for a in range(len(self.ms[state])):\n",
    "            self.ms[state][a] = (1 - self.decay) * self.ms[state][a] + self.epsilon_a * self.delta_func(action, a) * delta\n",
    "            self.policy[state] = self.softmax(self.ms[state])\n",
    "        return self.policy\n",
    "\n",
    "    def update_weights(self, reward, state, new_state):\n",
    "        if state == 0:\n",
    "            delta = reward + self.vs[new_state] - self.vs[state]\n",
    "        else:\n",
    "            delta = reward - self.vs[state]\n",
    "\n",
    "        self.vs[state] += self.epsilon_c * delta\n",
    "\n",
    "        return self.vs\n",
    "\n",
    "\n",
    "# policy evaluation for \"maze\" environment\n",
    "episodes = 1000\n",
    "epochs = 1000\n",
    "epsilon = 0.1\n",
    "\n",
    "env_normal = NormalEnvironment()\n",
    "agent_normal = ChoiceAgent(env=env_normal, epsilon_a=epsilon, policy_type=\"actor_critic\")\n",
    "v_hist = np.zeros((epochs, episodes, 3))\n",
    "policy_hist = np.zeros((epochs, episodes, 7))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    agent_normal.reset()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        for step in range(2):\n",
    "            if env_normal.terminated == False:\n",
    "                state = env_normal.state\n",
    "                v = agent_normal.vs\n",
    "                if state == 0:\n",
    "                    action = np.random.choice(np.arange(3), p=agent_normal.policy[state])\n",
    "                else:\n",
    "                    action = np.random.choice(np.arange(2), p=agent_normal.policy[state])\n",
    "                reward = env_normal.take_step(action)\n",
    "                new_state = env_normal.state\n",
    "                policy = agent_normal.update_policy(state, reward, new_state, action)\n",
    "                if new_state != 0:\n",
    "                    delta = reward + agent_normal.values[new_state] - agent_normal.values[state]\n",
    "                else:\n",
    "                    delta = reward - agent_normal.values[state]\n",
    "\n",
    "                agent_normal.values[state] += agent_normal.epsilon_c * delta\n",
    "\n",
    "        v_hist[epoch,episode] = v\n",
    "        policy_hist[epoch, episode] = np.concatenate((policy[0], policy[1], policy[2]))\n",
    "        env_normal.new_ep()\n",
    "\n",
    "# plot policy evaluation for \"normal\" environment\n",
    "means_per_epoch_1_pe_normal = np.mean(v_hist[:, :, 0], axis=0)\n",
    "plt.plot(range(epochs), v_hist[0, :, 0])\n",
    "plt.plot(range(epochs), means_per_epoch_1_pe_normal)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=1) - Normal\")\n",
    "plt.show()\n",
    "\n",
    "means_per_epoch_2_pe_normal = np.mean(v_hist[:, :, 1], axis=0)\n",
    "plt.plot(range(epochs), v_hist[0, :, 1])\n",
    "plt.plot(range(epochs), means_per_epoch_2_pe_normal)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=2) - Normal\")\n",
    "plt.show()\n",
    "\n",
    "means_per_epoch_3_pe_normal = np.mean(v_hist[:, :, 2], axis=0)\n",
    "plt.plot(range(epochs), v_hist[0, :, 2])\n",
    "plt.plot(range(epochs), means_per_epoch_3_pe_normal)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=3) - Normal\")\n",
    "plt.show()\n",
    "\n",
    "# policy evaluation for \"maze\" environment\n",
    "episodes = 1000\n",
    "epochs = 1000\n",
    "epsilon = 0.1\n",
    "\n",
    "env_maze = MazeEnvironment()\n",
    "agent_maze = MazeAgent(env=env_maze, epsilon_a=epsilon, policy_type=\"actor_critic\")\n",
    "v_over_epochs_pe_maze = np.zeros((epochs, episodes, 3))\n",
    "for epoch in range(epochs):\n",
    "    agent_maze.reset()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        for step in range(2):\n",
    "            if env_maze.terminated == False:\n",
    "                state = env_maze.get_state()\n",
    "                vs = agent_maze.critic()\n",
    "                action = agent_maze.actor()\n",
    "                reward = env_maze.take_step(action)\n",
    "                new_state = env_maze.get_state()\n",
    "                agent_maze.update_weights(reward, state, new_state)\n",
    "\n",
    "        v_over_epochs_pe_maze[epoch, episode] = vs\n",
    "        env_maze.new_ep()\n",
    "\n",
    "# plot policy evaluation for \"maze\" environment\n",
    "means_per_epoch_1_pe_maze = np.mean(v_over_epochs_pe_maze[:, :, 0], axis=0)\n",
    "plt.plot(range(epochs), v_over_epochs_pe_maze[0, :, 0])\n",
    "plt.plot(range(epochs), means_per_epoch_1_pe_maze)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=1) - Maze\")\n",
    "plt.show()\n",
    "\n",
    "means_per_epoch_2_pe_maze = np.mean(v_over_epochs_pe_maze[:, :, 1], axis=0)\n",
    "plt.plot(range(epochs), v_over_epochs_pe_maze[0, :, 1])\n",
    "plt.plot(range(epochs), means_per_epoch_2_pe_maze)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=2) - Maze\")\n",
    "plt.show()\n",
    "\n",
    "means_per_epoch_3_pe_maze = np.mean(v_over_epochs_pe_maze[:, :, 2], axis=0)\n",
    "plt.plot(range(epochs), v_over_epochs_pe_maze[0, :, 2])\n",
    "plt.plot(range(epochs), means_per_epoch_3_pe_maze)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"v(u=3) - Maze\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
